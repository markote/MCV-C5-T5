# -*- coding: utf-8 -*-
"""Domain Shift Fine Tunning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q5p8f6BDDYjekYZfyUAoHYOYUSBzHeYD

# **SETUP**
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pydicom

!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'

import os
import json
import numpy as np
import pydicom
import cv2
from tqdm import tqdm  # For progress tracking
from scipy.ndimage import median_filter  # For 2D median filtering

import re
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation

from detectron2.data.datasets import register_coco_instances
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2.config import get_cfg
from detectron2.engine import DefaultTrainer, DefaultPredictor
from detectron2.engine import hooks
from detectron2.data import build_detection_test_loader
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2 import model_zoo

import os
import math


from matplotlib import rc
rc('animation', html='jshtml')
from IPython import display

"""# **DATASET**"""

def convert_medical_dataset_to_coco(
    input_path,
    output_path,
    apply_denoising=False,
    only_annotations=True
):
    """
    Converts a medical dataset with DICOM images and NumPy mask files
    into COCO-compatible format, while also converting images to grayscale JPG.

    For each slice, this version calculates a single bounding box that covers all
    non-zero mask pixels, ignoring connected components.

    This function will produce a separate COCO JSON file for each split:
      - train_coco_annotations.json
      - test_coco_annotations.json
      - validation_coco_annotations.json

    In the resulting JSON, 'file_name' only contains the actual file name
    (e.g. "study1_5.jpg"), excluding the 'train', 'test', or 'validation' folder prefix.

    Args:
        input_path (str): Path to the dataset containing 'train', 'test', and 'validation' folders.
        output_path (str): Path to save the output COCO JSON annotations and images.
        apply_denoising (bool): If True, applies a 2D median filter to each slice for denoising.
        only_annotations (bool): If True, do not save JPG images (only produce annotations).
    """

    # We will process each split separately and produce a separate JSON file
    splits = ["train", "test", "validation"]
    os.makedirs(output_path, exist_ok=True)

    # Common categories for all splits (if you have more, add them here)
    categories = [{"id": 1, "name": "heart"}]

    for split in splits:
        # Initialize COCO data structure for this specific split
        coco_data = {
            "images": [],
            "annotations": [],
            "categories": categories
        }

        # Counters for this split
        ann_id = 1
        image_id = 1

        # Input folders for images and labels
        image_folder = os.path.join(input_path, split, "images")
        label_folder = os.path.join(input_path, split, "labels")

        # Output folder for images (if saving)
        output_image_folder = os.path.join(output_path, split)
        os.makedirs(output_image_folder, exist_ok=True)  # Create output directory if needed

        # Process each study folder inside the images directory
        study_folders = os.listdir(image_folder)
        print(f"Processing {split} split ({len(study_folders)} studies)...")

        for study_id in tqdm(study_folders, desc=f"Processing {split}"):
            study_img_path = os.path.join(image_folder, study_id)

            # By convention in your dataset, labels are under 'study_id + "_heart"'
            study_label_path = os.path.join(label_folder, study_id + '_heart')

            if not os.path.isdir(study_img_path):
                continue  # Skip if it's not a directory (safety check)

            # Collect all DICOM files, sorted by slice index
            slice_files = sorted(
                [f for f in os.listdir(study_img_path) if f.endswith(".dcm")],
                key=lambda x: int(x.split('.')[0])
            )
            num_files = len(slice_files)

            # Process each slice
            for file_name in slice_files:
                slice_index = file_name.split('.')[0]  # Extract slice number
                dicom_path = os.path.join(study_img_path, file_name)
                mask_path = os.path.join(study_label_path, f"slice_{num_files - int(slice_index)}.npy")

                # Ensure the corresponding mask exists
                if not os.path.exists(mask_path):
                    continue

                # Load the DICOM
                dicom = pydicom.dcmread(dicom_path)
                pixel_array = dicom.pixel_array.astype(np.float32)

                # Rescale to HU (if tags exist)
                slope = float(dicom.RescaleSlope) if "RescaleSlope" in dicom else 1
                intercept = float(dicom.RescaleIntercept) if "RescaleIntercept" in dicom else 0
                hu_image = (pixel_array * slope) + intercept  # Convert to HU

                # Clip and normalize to [0, 255]
                hu_image = np.clip(hu_image, -150, 250)
                normalized_image = ((hu_image + 150) / 400.0) * 255.0
                normalized_image = normalized_image.astype(np.uint8)

                # Optional denoising
                if apply_denoising:
                    normalized_image = median_filter(normalized_image, size=3)

                # Save the grayscale JPG if requested
                file_base_name = f"{study_id}_{slice_index}.jpg"  # e.g. "study1_5.jpg"
                if not only_annotations:
                    output_img_path = os.path.join(output_image_folder, file_base_name)
                    cv2.imwrite(output_img_path, normalized_image)

                # Determine the dimensions (width, height)
                img_width, img_height = normalized_image.shape[::-1]

                # Add the 'image' entry in COCO format
                coco_data["images"].append({
                    "id": image_id,
                    # We only store the file's base name (no 'train/test/validation' prefix)
                    "file_name": file_base_name,
                    "width": int(img_width),
                    "height": int(img_height)
                })

                # Load the binary mask
                mask = np.load(mask_path).astype(np.uint8)
                mask[mask > 0] = 1  # Ensure binary

                # plt.imshow(normalized_image, cmap='gray')
                # plt.imshow(mask, alpha=0.25)
                # plt.show()

                # Compute one bounding box covering all non-zero pixels
                rows, cols = np.where(mask == 1)
                if len(rows) > 0 and len(cols) > 0:
                    y_min, y_max = rows.min(), rows.max()
                    x_min, x_max = cols.min(), cols.max()
                    w = x_max - x_min + 1
                    h = y_max - y_min + 1
                    area = w * h

                    if area > 0:
                        bbox = [int(x_min), int(y_min), int(w), int(h)]
                        coco_data["annotations"].append({
                            "id": ann_id,
                            "image_id": image_id,
                            "category_id": 1,  # Single class: "heart"
                            "bbox": bbox,
                            "area": int(area),
                            "iscrowd": 0,
                            "segmentation": []
                        })
                        ann_id += 1

                image_id += 1

        # After processing all studies in this split, save the COCO JSON
        output_json_name = f"{split}_coco_annotations.json"
        coco_json_path = os.path.join(output_path, output_json_name)

        with open(coco_json_path, "w") as f:
            json.dump(coco_data, f, indent=4)

        print(f"\n{split.capitalize()} split annotations saved to {coco_json_path}")

# convert_medical_dataset_to_coco(
#     input_path="/content/drive/MyDrive/STUDIES/HEART_SEGMENTATION_DATASET",
#     output_path="/content/drive/MyDrive/## MCV/C5/Week1/Heart Detection Dataset",
#     apply_denoising=True,
#     only_annotations=False
# )

def visualize_study(
    dataset_name,
    coco_json_path,
    images_folder,
    study_id,
    box_thickness=2,
    box_color=(0, 255, 0),
    interval=500
):
    """
    Registers a COCO-format dataset in Detectron2, retrieves all images for a given study
    (based on filenames like <study_id>_<slice_index>.jpg), draws bounding boxes, and
    displays them as an animation in a Jupyter/Colab notebook.

    Args:
        dataset_name (str): Name to register the dataset in Detectron2.
        coco_json_path (str): Path to the COCO JSON annotation file.
        images_folder (str): Path to the folder containing your dataset's images.
        study_id (str): Study identifier used in filenames (e.g., "study123").
        box_thickness (int): Thickness (in px) of the bounding box lines.
        box_color (tuple): Bounding box color in RGB format, e.g. (255, 0, 0) for red.
        interval (int): Delay between frames in milliseconds (controls animation speed).
    """

    # 1. Register the dataset with Detectron2 (if not done already)
    register_coco_instances(dataset_name, {}, coco_json_path, images_folder)

    # 2. Retrieve dataset metadata/dicts
    metadata = MetadataCatalog.get(dataset_name)
    dataset_dicts = DatasetCatalog.get(dataset_name)
    if not dataset_dicts:
        print(f"No samples found in dataset: {dataset_name}")
        return

    # 3. Find & sort images for the specific study
    #    We assume the filename is <study_id>_<slice_index>.jpg
    #    e.g. "study123_1.jpg", "study123_2.jpg"
    study_records = []
    pattern = re.compile(rf"^{re.escape(study_id)}_(\d+)\.jpg$")

    for sample in dataset_dicts:
        filename = os.path.basename(sample["file_name"])  # "study123_10.jpg"
        match = pattern.match(filename)
        if match:
            slice_idx = int(match.group(1))
            study_records.append((slice_idx, sample))

    # Sort by numeric slice index
    study_records.sort(key=lambda x: x[0])

    if not study_records:
        print(f"No images found for study '{study_id}' in dataset '{dataset_name}'.")
        return

    # 4. Prepare frames (draw bounding boxes) for animation
    frames = []
    for slice_idx, sample in study_records:
        # Read image
        img_path = sample["file_name"]
        img = cv2.imread(img_path)
        if img is None:
            print(f"Warning: Could not read image file: {img_path}")
            continue

        # If 'annotations' is in sample, draw bounding boxes
        # (In a COCO dataset, each sample typically has an 'annotations' field)
        if "annotations" in sample:
            for ann in sample["annotations"]:
                # x_min, y_min, width, height
                x, y, w, h = ann["bbox"]
                x2, y2 = int(x + w), int(y + h)

                # Convert box_color (RGB) to BGR for OpenCV
                bgr_color = (box_color[2], box_color[1], box_color[0])
                cv2.rectangle(
                    img,
                    (int(x), int(y)),
                    (x2, y2),
                    color=bgr_color,
                    thickness=box_thickness
                )

        # Convert BGR -> RGB for matplotlib
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        frames.append(img_rgb)

    if not frames:
        print(f"No valid frames to display for study '{study_id}'.")
        return

    # 5. Create a matplotlib animation
    fig, ax = plt.subplots()
    # We'll display the first frame initially
    im = ax.imshow(frames[0])
    ax.set_title(f"Study: {study_id}, Slice: {study_records[0][0]}")
    ax.axis("off")

    def update(frame_idx):
        # Update the image data to the new frame
        im.set_data(frames[frame_idx])

        # Update title with slice index
        current_slice = study_records[frame_idx][0]
        ax.set_title(f"Study: {study_id}, Slice: {current_slice}")
        return [im]

    ani = animation.FuncAnimation(
        fig,
        update,
        frames=len(frames),
        interval=interval,
        blit=False
    )
    ani.save("my_animation.gif", writer="pillow", fps=4)
    display.display(ani)
    plt.close()

dataset_name = "my_heart_dataset_2"
coco_json = "/content/drive/MyDrive/## MCV/C5/Week1/Heart Detection Dataset/train_coco_annotations.json"
images_dir = "/content/drive/MyDrive/## MCV/C5/Week1/Heart Detection Dataset/train"
study_id = "ID00007637202177411956430"

visualize_study(dataset_name, coco_json, images_dir, study_id=study_id)

"""# FINETUNNING"""

def print_bbox_area_stats(dataset_name):
    """
    Computes and prints bounding box area statistics (min, median, max)
    for the specified Detectron2 dataset.

    Args:
        dataset_name (str): Name of the dataset (must be registered in DatasetCatalog).
    """
    dataset_dicts = DatasetCatalog.get(dataset_name)
    areas = []

    # Gather bounding box areas
    for record in dataset_dicts:
        for ann in record.get("annotations", []):
            x, y, w, h = ann["bbox"]
            areas.append(w * h)

    # Convert to NumPy array for easy stats
    areas = np.array(areas, dtype=np.float32)
    if len(areas) == 0:
        print(f"No bounding boxes found in dataset '{dataset_name}'.")
        return

    # Print stats
    print(f"Bounding box area stats for '{dataset_name}':")
    print("  min:", areas.min())
    print("  median:", np.median(areas))
    print("  max:", areas.max())

def train_faster_rcnn(
    train_name,
    val_name,
    test_name,
    train_json_path,
    val_json_path,
    test_json_path,
    train_images_dir,
    val_images_dir,
    test_images_dir,
    backbone="R_50",
    steps_per_epoch=200,
    epochs=10,
    batch_size=2,
    base_lr=0.00025,
    eval_period=1,
    output_dir="./output"
):
    """
    Fine-tunes a Faster R-CNN on a custom COCO-format dataset using Detectron2.
    Logs training/validation loss per iteration and computes mAP@0.5 on validation
    after each epoch (if eval_period=1). Finally evaluates on the test set.
    Also saves the config (config.yaml) to the output directory for later reference.
    """

    # 1. Register the COCO-format datasets (train, val, test)
    register_coco_instances(train_name, {}, train_json_path, train_images_dir)
    register_coco_instances(val_name, {}, val_json_path, val_images_dir)
    register_coco_instances(test_name, {}, test_json_path, test_images_dir)

    # 2. Prepare a configuration from Detectron2's Model Zoo
    backbone_map = {
        "R_50": "COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml",
        "R_101": "COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml",
    }
    if backbone not in backbone_map:
        raise ValueError(f"Unknown backbone '{backbone}'. Supported: {list(backbone_map.keys())}")

    cfg = get_cfg()
    cfg.merge_from_file(model_zoo.get_config_file(backbone_map[backbone]))
    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(backbone_map[backbone])

    # Configure anchors, flips, cropping, resizing
    cfg.MODEL.ANCHOR_GENERATOR.SIZES = [
        [64], [128], [256], [512], [1024]
    ]
    cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[1.0]]

    cfg.INPUT.RANDOM_FLIP = "horizontal"

    cfg.INPUT.CROP.ENABLED = True
    cfg.INPUT.CROP.TYPE = "relative_range"
    cfg.INPUT.CROP.SIZE = [0.8, 0.8]

    cfg.INPUT.MIN_SIZE_TRAIN = (512,)
    cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING = "choice"
    cfg.INPUT.MAX_SIZE_TRAIN = 512

    cfg.INPUT.MIN_SIZE_TEST = 512
    cfg.INPUT.MAX_SIZE_TEST = 512

    # 3. Update config for custom dataset
    cfg.DATASETS.TRAIN = (train_name,)
    cfg.DATASETS.TEST = (val_name,)
    cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False
    cfg.DATALOADER.NUM_WORKERS = 4

    # Number of classes for your dataset
    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1

    # 4. Approximate steps per epoch
    train_dicts = DatasetCatalog.get(train_name)
    num_images = len(train_dicts)
    steps_per_epoch = min(math.ceil(num_images / batch_size), steps_per_epoch)
    total_iterations = epochs * steps_per_epoch
    cfg.SOLVER.MAX_ITER = total_iterations

    cfg.SOLVER.BASE_LR = base_lr
    cfg.SOLVER.IMS_PER_BATCH = batch_size

    cfg.TEST.EVAL_PERIOD = eval_period * steps_per_epoch
    cfg.SOLVER.CHECKPOINT_PERIOD = steps_per_epoch

    # Output directory
    cfg.OUTPUT_DIR = output_dir
    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)

    # 5. Create custom trainer
    class CustomTrainer(DefaultTrainer):
        @classmethod
        def build_evaluator(cls, cfg, dataset_name):
            return COCOEvaluator(dataset_name, cfg, True, output_dir=cfg.OUTPUT_DIR)

    trainer = CustomTrainer(cfg)
    trainer.resume_or_load(resume=False)

    # ---- Save the config to a file (config.yaml) in OUTPUT_DIR ----
    config_path = os.path.join(cfg.OUTPUT_DIR, "config.yaml")
    with open(config_path, "w") as f:
        f.write(cfg.dump())
    print(f"Config saved to {config_path}")

    # 6. Train
    print("Starting training...")
    trainer.train()
    print("Training complete!")

    # 7. Evaluate on Test Split
    print("Evaluating on test set...")
    evaluator_test = COCOEvaluator(test_name, cfg, False, output_dir=cfg.OUTPUT_DIR)
    test_loader = build_detection_test_loader(cfg, test_name)
    test_results = inference_on_dataset(trainer.model, test_loader, evaluator_test)
    print("Test results:", test_results)

    return trainer.model, cfg

model, cfg = train_faster_rcnn(
    train_name="my_dataset_train",
    val_name="my_dataset_val",
    test_name="my_dataset_test",
    train_json_path="/content/drive/MyDrive/## MCV/C5/Week1/Heart Detection Dataset/train_coco_annotations.json",
    val_json_path="/content/drive/MyDrive/## MCV/C5/Week1/Heart Detection Dataset/validation_coco_annotations.json",
    test_json_path="/content/drive/MyDrive/## MCV/C5/Week1/Heart Detection Dataset/test_coco_annotations.json",
    train_images_dir="/content/drive/MyDrive/## MCV/C5/Week1/Heart Detection Dataset/train",
    val_images_dir="/content/drive/MyDrive/## MCV/C5/Week1/Heart Detection Dataset/validation",
    test_images_dir="/content/drive/MyDrive/## MCV/C5/Week1/Heart Detection Dataset/test",
    backbone="R_50",
    steps_per_epoch=250,
    epochs=10,
    batch_size=16,
    base_lr=0.0001,
    eval_period=1,
    output_dir="/content/drive/MyDrive/## MCV/C5/Week1/output/test7_50"
)

print_bbox_area_stats("my_dataset_train")

